{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import keras\n",
    "import imageio\n",
    "from scipy import signal\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from skimage.transform import resize\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------\n",
      "finding files: model, morphology and test data\n",
      "-----------------------------------------------\n",
      "model found          : \"NMDA_TCN__DWT_7_128_153__model.h5\"\n",
      "model metadata found : \"NMDA_TCN__DWT_7_128_153__training.pickle\"\n",
      "morphology found     : \"morphology_dict.pickle\"\n",
      "number of test files is 12\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "save_figures = False\n",
    "file_ending = '.png'\n",
    "model_string = 'NMDA'\n",
    "\n",
    "dataset_folder = 'data'\n",
    "\n",
    "models_folder     = os.path.join(dataset_folder, 'Models')\n",
    "morphology_folder = os.path.join(dataset_folder, 'Morphology')\n",
    "test_data_folder  = os.path.join(dataset_folder, 'Data_test')\n",
    "auxiliary_folder  = os.path.join(dataset_folder, 'Auxiliary')\n",
    "\n",
    "model_filename           = os.path.join(models_folder, 'NMDA_TCN__DWT_7_128_153__model.h5')\n",
    "model_metadata_filename  = os.path.join(models_folder, 'NMDA_TCN__DWT_7_128_153__training.pickle')\n",
    "morphology_filename      = os.path.join(morphology_folder, 'morphology_dict.pickle')\n",
    "NN_illustration_filename = os.path.join(auxiliary_folder, 'TCN_7_layers.png')\n",
    "test_files               = sorted(glob.glob(os.path.join(test_data_folder, '*_128_simulationRuns*_6_secDuration_*')))\n",
    "\n",
    "print('-----------------------------------------------')\n",
    "print('finding files: model, morphology and test data')\n",
    "print('-----------------------------------------------')\n",
    "print('model found          : \"%s\"' %(model_filename.split('/')[-1]))\n",
    "print('model metadata found : \"%s\"' %(model_metadata_filename.split('/')[-1]))\n",
    "print('morphology found     : \"%s\"' %(morphology_filename.split('/')[-1]))\n",
    "print('number of test files is %d' %(len(test_files)))\n",
    "print('-----------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parse_multiple_sim_experiment_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5ab3e1c18b24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mexperiment_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mparse_multiple_sim_experiment_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'parse_multiple_sim_experiment_files' is not defined"
     ]
    }
   ],
   "source": [
    "experiment_dict = pickle.load(open(test_files[0], \"rb\" ), encoding='latin1')\n",
    "parse_multiple_sim_experiment_files(experiment_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##%% helper functions\n",
    "\n",
    "def bin2dict(bin_spikes_matrix):\n",
    "    spike_row_inds, spike_times = np.nonzero(bin_spikes_matrix)\n",
    "    row_inds_spike_times_map = {}\n",
    "    for row_ind, syn_time in zip(spike_row_inds,spike_times):\n",
    "        if row_ind in row_inds_spike_times_map.keys():\n",
    "            row_inds_spike_times_map[row_ind].append(syn_time)\n",
    "        else:\n",
    "            row_inds_spike_times_map[row_ind] = [syn_time]\n",
    "\n",
    "    return row_inds_spike_times_map\n",
    "\n",
    "\n",
    "def dict2bin(row_inds_spike_times_map, num_segments, sim_duration_ms):\n",
    "    \n",
    "    bin_spikes_matrix = np.zeros((num_segments, sim_duration_ms), dtype='bool')\n",
    "    for row_ind in row_inds_spike_times_map.keys():\n",
    "        for spike_time in row_inds_spike_times_map[row_ind]:\n",
    "            bin_spikes_matrix[row_ind,spike_time] = 1.0\n",
    "    \n",
    "    return bin_spikes_matrix\n",
    "\n",
    "def parse_sim_experiment_file(sim_experiment_file):\n",
    "    \n",
    "    print('-----------------------------------------------------------------')\n",
    "    print(\"loading file: '\" + sim_experiment_file.split(\"\\\\\")[-1] + \"'\")\n",
    "    loading_start_time = time.time()\n",
    "    experiment_dict = pickle.load(open(sim_experiment_file, \"rb\" ), encoding='latin1')\n",
    "    \n",
    "    # gather params\n",
    "    num_simulations = len(experiment_dict['Results']['listOfSingleSimulationDicts'])\n",
    "    num_segments    = len(experiment_dict['Params']['allSegmentsType'])\n",
    "    sim_duration_ms = experiment_dict['Params']['totalSimDurationInSec'] * 1000\n",
    "    num_ex_synapses  = num_segments\n",
    "    num_inh_synapses = num_segments\n",
    "    num_synapses = num_ex_synapses + num_inh_synapses\n",
    "    \n",
    "    # collect X, y_spike, y_soma\n",
    "    X = np.zeros((num_synapses,sim_duration_ms,num_simulations), dtype='bool')\n",
    "    y_spike = np.zeros((sim_duration_ms,num_simulations), dtype=np.float32)\n",
    "    y_soma  = np.zeros((sim_duration_ms,num_simulations), dtype=np.float32)\n",
    "    for k, sim_dict in enumerate(experiment_dict['Results']['listOfSingleSimulationDicts']):\n",
    "        X_ex  = dict2bin(sim_dict['exInputSpikeTimes'] , num_segments, sim_duration_ms)\n",
    "        X_inh = dict2bin(sim_dict['inhInputSpikeTimes'], num_segments, sim_duration_ms)\n",
    "        X[:,:,k] = np.vstack((X_ex,X_inh))\n",
    "        spike_times = (sim_dict['outputSpikeTimes'].astype(float) - 0.5).astype(int)\n",
    "        y_spike[spike_times,k] = 1.0\n",
    "        y_soma[:,k] = sim_dict['somaVoltageLowRes']\n",
    "\n",
    "    loading_duration_sec = time.time() - loading_start_time \n",
    "    print('loading took %.3f seconds' %(loading_duration_sec))\n",
    "    print('-----------------------------------------------------------------')\n",
    "\n",
    "    return X, y_spike, y_soma\n",
    "\n",
    "\n",
    "def parse_multiple_sim_experiment_files(sim_experiment_files):\n",
    "    \n",
    "    for k, sim_experiment_file in enumerate(sim_experiment_files):\n",
    "        X_curr, y_spike_curr, y_soma_curr = parse_sim_experiment_file(sim_experiment_file)\n",
    "        \n",
    "        if k == 0:\n",
    "            X       = X_curr\n",
    "            y_spike = y_spike_curr\n",
    "            y_soma  = y_soma_curr\n",
    "        else:\n",
    "            X       = np.dstack((X,X_curr))\n",
    "            y_spike = np.hstack((y_spike,y_spike_curr))\n",
    "            y_soma  = np.hstack((y_soma,y_soma_curr))\n",
    "\n",
    "    return X, y_spike, y_soma\n",
    "\n",
    "\n",
    "def calc_AUC_at_desired_FP(y_test, y_test_hat, desired_false_positive_rate=0.01):\n",
    "    fpr, tpr, thresholds = roc_curve(y_test.ravel(), y_test_hat.ravel())\n",
    "\n",
    "    linear_spaced_FPR = np.linspace(0,1,num=20000)\n",
    "    linear_spaced_TPR = np.interp(linear_spaced_FPR, fpr, tpr)\n",
    "    \n",
    "    desired_fp_ind = min(max(1,np.argmin(abs(linear_spaced_FPR-desired_false_positive_rate))),linear_spaced_TPR.shape[0]-1)\n",
    "    \n",
    "    return linear_spaced_TPR[:desired_fp_ind].mean()\n",
    "\n",
    "\n",
    "def calc_TP_at_desired_FP(y_test, y_test_hat, desired_false_positive_rate=0.0025):\n",
    "    fpr, tpr, thresholds = roc_curve(y_test.ravel(), y_test_hat.ravel())\n",
    "    \n",
    "    desired_fp_ind = np.argmin(abs(fpr-desired_false_positive_rate))\n",
    "    if desired_fp_ind == 0:\n",
    "        desired_fp_ind = 1\n",
    "\n",
    "    return tpr[desired_fp_ind]\n",
    "\n",
    "\n",
    "def exctract_key_results(y_spikes_GT, y_spikes_hat, y_soma_GT, y_soma_hat, desired_FP_list=[0.0025,0.0100]):\n",
    "    \n",
    "    # evaluate the model and save the results\n",
    "    print('----------------------------------------------------------------------------------------')\n",
    "    print('calculating key results...')\n",
    "    \n",
    "    evaluation_start_time = time.time()\n",
    "    \n",
    "    # store results in the hyper param dict and return it\n",
    "    evaluations_results_dict = {}\n",
    "    \n",
    "    for desired_FP in desired_FP_list:\n",
    "        TP_at_desired_FP  = calc_TP_at_desired_FP(y_spikes_GT, y_spikes_hat, desired_false_positive_rate=desired_FP)\n",
    "        AUC_at_desired_FP = calc_AUC_at_desired_FP(y_spikes_GT, y_spikes_hat, desired_false_positive_rate=desired_FP)\n",
    "        print('-----------------------------------')\n",
    "        print('TP  at %.4f FP rate = %.4f' %(desired_FP, TP_at_desired_FP))\n",
    "        print('AUC at %.4f FP rate = %.4f' %(desired_FP, AUC_at_desired_FP))\n",
    "        TP_key_string = 'TP @ %.4f FP' %(desired_FP)\n",
    "        evaluations_results_dict[TP_key_string] = TP_at_desired_FP\n",
    "    \n",
    "        AUC_key_string = 'AUC @ %.4f FP' %(desired_FP)\n",
    "        evaluations_results_dict[AUC_key_string] = AUC_at_desired_FP\n",
    "    \n",
    "    print('--------------------------------------------------')\n",
    "    fpr, tpr, thresholds = roc_curve(y_spikes_GT.ravel(), y_spikes_hat.ravel())\n",
    "    AUC_score = auc(fpr, tpr)\n",
    "    print('AUC = %.4f' %(AUC_score))\n",
    "    print('--------------------------------------------------')\n",
    "    \n",
    "    soma_explained_variance_percent = 100.0*explained_variance_score(y_soma_GT.ravel(),y_soma_hat.ravel())\n",
    "    soma_RMSE = np.sqrt(MSE(y_soma_GT.ravel(),y_soma_hat.ravel()))\n",
    "    soma_MAE  = MAE(y_soma_GT.ravel(),y_soma_hat.ravel())\n",
    "    \n",
    "    print('--------------------------------------------------')\n",
    "    print('soma explained_variance percent = %.2f%s' %(soma_explained_variance_percent, '%'))\n",
    "    print('soma RMSE = %.3f [mV]' %(soma_RMSE))\n",
    "    print('soma MAE = %.3f [mV]' %(soma_MAE))\n",
    "    print('--------------------------------------------------')\n",
    "    \n",
    "    evaluations_results_dict['AUC'] = AUC_score\n",
    "    evaluations_results_dict['soma_explained_variance_percent'] = soma_explained_variance_percent\n",
    "    evaluations_results_dict['soma_RMSE'] = soma_RMSE\n",
    "    evaluations_results_dict['soma_MAE'] = soma_MAE\n",
    "    \n",
    "    evaluation_duration_min = (time.time() - evaluation_start_time)/60\n",
    "    print('finished evaluation. time took to evaluate results is %.2f minutes' %(evaluation_duration_min))\n",
    "    print('----------------------------------------------------------------------------------------')\n",
    "    \n",
    "    return evaluations_results_dict\n",
    "\n",
    "\n",
    "def filter_and_exctract_key_results(y_spikes_GT, y_spikes_hat, y_soma_GT, y_soma_hat, desired_FP_list=[0.0025,0.0100], \n",
    "                                                                                      ignore_time_at_start_ms=500, \n",
    "                                                                                      num_spikes_per_sim=[0,24]):\n",
    "\n",
    "    time_points_to_eval = np.arange(y_spikes_GT.shape[1]) >= ignore_time_at_start_ms\n",
    "    simulations_to_eval = np.logical_and((y_spikes_GT.sum(axis=1) >= num_spikes_per_sim[0]),(y_spikes_GT.sum(axis=1) <= num_spikes_per_sim[1]))\n",
    "    \n",
    "    print('total amount of simualtions is %d' %(y_spikes_GT.shape[0]))\n",
    "    print('percent of simulations kept = %.2f%s' %(100*simulations_to_eval.mean(),'%'))\n",
    "    \n",
    "    y_spikes_GT_to_eval  = y_spikes_GT[simulations_to_eval,:][:,time_points_to_eval]\n",
    "    y_spikes_hat_to_eval = y_spikes_hat[simulations_to_eval,:][:,time_points_to_eval]\n",
    "    y_soma_GT_to_eval    = y_soma_GT[simulations_to_eval,:][:,time_points_to_eval]\n",
    "    y_soma_hat_to_eval   = y_soma_hat[simulations_to_eval,:][:,time_points_to_eval]\n",
    "    \n",
    "    return exctract_key_results(y_spikes_GT_to_eval, y_spikes_hat_to_eval, y_soma_GT_to_eval, y_soma_hat_to_eval, desired_FP_list=desired_FP_list)\n",
    "\n",
    "\n",
    "def draw_weights(first_layer_weights, selected_filter_ind, set_ylabel, ax00,ax10, ax01,ax11, ax02,ax12):\n",
    "\n",
    "    time_span, _, num_filters = first_layer_weights.shape\n",
    "    \n",
    "    weight_granularity = 0.06\n",
    "    time_granularity = 20\n",
    "    max_time_to_show = 40\n",
    "    \n",
    "    if use_filtered:\n",
    "        first_layer_weights_filtered = signal.convolve(first_layer_weights, (1.0/filter_size)*np.ones((filter_size,1,1)), mode='valid')\n",
    "        first_layer_weights = first_layer_weights_filtered\n",
    "    \n",
    "    if first_layer_weights.shape[0] >= max_time_to_show:\n",
    "        first_layer_weights = first_layer_weights[:max_time_to_show]\n",
    "    \n",
    "    num_segments =  639\n",
    "    basal_cutoff =  262\n",
    "    tuft_cutoff  = [366,559]\n",
    "\n",
    "    # invert if needed\n",
    "    exc_sum = first_layer_weights[:12,:num_segments,selected_filter_ind].sum()\n",
    "    inh_sum = first_layer_weights[:12,num_segments:,selected_filter_ind].sum()\n",
    "    exc_minus_inh = exc_sum - inh_sum\n",
    "    \n",
    "    if exc_minus_inh < 0:\n",
    "        first_layer_weights = -first_layer_weights\n",
    "    \n",
    "    upper_limit = max(np.percentile(abs(first_layer_weights[:,:,selected_filter_ind]),99.95),np.percentile(abs(first_layer_weights[:,:,selected_filter_ind]),0.05))\n",
    "    xlims = [-5*int(first_layer_weights.shape[0]/5),0]\n",
    "    \n",
    "    ex_basal_syn_inds    = np.arange(basal_cutoff)\n",
    "    ex_oblique_syn_inds  = np.hstack((np.arange(basal_cutoff,tuft_cutoff[0]),np.arange(tuft_cutoff[1],num_segments)))\n",
    "    ex_tuft_syn_inds     = np.arange(tuft_cutoff[0],tuft_cutoff[1])\n",
    "    inh_basal_syn_inds   = num_segments + ex_basal_syn_inds\n",
    "    inh_oblique_syn_inds = num_segments + ex_oblique_syn_inds\n",
    "    inh_tuft_syn_inds    = num_segments + ex_tuft_syn_inds\n",
    "    \n",
    "    basal_weights_example_filter_ex  = np.fliplr(first_layer_weights[:,ex_basal_syn_inds,selected_filter_ind].T)\n",
    "    basal_weights_example_filter_inh = np.fliplr(first_layer_weights[:,inh_basal_syn_inds,selected_filter_ind].T)\n",
    "    basal_weights_example_filter     = np.concatenate((basal_weights_example_filter_ex,basal_weights_example_filter_inh),axis=0)\n",
    "    oblique_weights_example_filter_ex  = np.fliplr(first_layer_weights[:,ex_oblique_syn_inds,selected_filter_ind].T)\n",
    "    oblique_weights_example_filter_inh = np.fliplr(first_layer_weights[:,inh_oblique_syn_inds,selected_filter_ind].T)\n",
    "    oblique_weights_example_filter     = np.concatenate((oblique_weights_example_filter_ex, oblique_weights_example_filter_inh),axis=0)\n",
    "    tuft_weights_example_filter_ex  = np.fliplr(first_layer_weights[:,ex_tuft_syn_inds,selected_filter_ind].T)\n",
    "    tuft_weights_example_filter_inh = np.fliplr(first_layer_weights[:,inh_tuft_syn_inds,selected_filter_ind].T)\n",
    "    tuft_weights_example_filter     = np.concatenate((tuft_weights_example_filter_ex,tuft_weights_example_filter_inh),axis=0)\n",
    "    \n",
    "    time_axis = -np.arange(first_layer_weights.shape[0])\n",
    "    \n",
    "    ##%% create nice figure\n",
    "    figure_xlims = xlims\n",
    "    figure_xlims[0] = max(-40, figure_xlims[0])\n",
    "    \n",
    "    ex_basal_syn_inds    = np.arange(basal_cutoff)\n",
    "    ex_oblique_syn_inds  = np.hstack((np.arange(basal_cutoff,tuft_cutoff[0]),np.arange(tuft_cutoff[1],num_segments)))\n",
    "    ex_tuft_syn_inds     = np.arange(tuft_cutoff[0],tuft_cutoff[1])\n",
    "    inh_basal_syn_inds   = num_segments + ex_basal_syn_inds\n",
    "    inh_oblique_syn_inds = num_segments + ex_oblique_syn_inds\n",
    "    inh_tuft_syn_inds    = num_segments + ex_tuft_syn_inds\n",
    "    \n",
    "    basal_weights_example_filter_ex  = np.fliplr(first_layer_weights[:,ex_basal_syn_inds,selected_filter_ind].T)\n",
    "    basal_weights_example_filter_inh = np.fliplr(first_layer_weights[:,inh_basal_syn_inds,selected_filter_ind].T)\n",
    "    basal_weights_example_filter     = np.concatenate((basal_weights_example_filter_ex,basal_weights_example_filter_inh),axis=0)\n",
    "    oblique_weights_example_filter_ex  = np.fliplr(first_layer_weights[:,ex_oblique_syn_inds,selected_filter_ind].T)\n",
    "    oblique_weights_example_filter_inh = np.fliplr(first_layer_weights[:,inh_oblique_syn_inds,selected_filter_ind].T)\n",
    "    oblique_weights_example_filter     = np.concatenate((oblique_weights_example_filter_ex, oblique_weights_example_filter_inh),axis=0)\n",
    "    tuft_weights_example_filter_ex  = np.fliplr(first_layer_weights[:,ex_tuft_syn_inds,selected_filter_ind].T)\n",
    "    tuft_weights_example_filter_inh = np.fliplr(first_layer_weights[:,inh_tuft_syn_inds,selected_filter_ind].T)\n",
    "    tuft_weights_example_filter     = np.concatenate((tuft_weights_example_filter_ex,tuft_weights_example_filter_inh),axis=0)\n",
    "    \n",
    "    combined_filter = np.concatenate((basal_weights_example_filter_ex,oblique_weights_example_filter_ex,tuft_weights_example_filter_ex,\n",
    "                                      basal_weights_example_filter_inh,oblique_weights_example_filter_inh,tuft_weights_example_filter_inh),axis=0)\n",
    "    \n",
    "    ##%% draw 2 x 3 (basal,oblique,tuft) matrix\n",
    "    ex_basal_syn_inds    = np.arange(basal_cutoff)\n",
    "    ex_oblique_syn_inds  = np.hstack((np.arange(basal_cutoff,tuft_cutoff[0]),np.arange(tuft_cutoff[1],num_segments)))\n",
    "    ex_tuft_syn_inds     = np.arange(tuft_cutoff[0],tuft_cutoff[1])\n",
    "    inh_basal_syn_inds   = num_segments + ex_basal_syn_inds\n",
    "    inh_oblique_syn_inds = num_segments + ex_oblique_syn_inds\n",
    "    inh_tuft_syn_inds    = num_segments + ex_tuft_syn_inds\n",
    "    \n",
    "    basal_weights_example_filter_ex  = np.fliplr(first_layer_weights[:,ex_basal_syn_inds,selected_filter_ind].T)\n",
    "    basal_weights_example_filter_inh = np.fliplr(first_layer_weights[:,inh_basal_syn_inds,selected_filter_ind].T)\n",
    "    basal_weights_example_filter     = np.concatenate((basal_weights_example_filter_ex,basal_weights_example_filter_inh),axis=0)\n",
    "    oblique_weights_example_filter_ex  = np.fliplr(first_layer_weights[:,ex_oblique_syn_inds,selected_filter_ind].T)\n",
    "    oblique_weights_example_filter_inh = np.fliplr(first_layer_weights[:,inh_oblique_syn_inds,selected_filter_ind].T)\n",
    "    oblique_weights_example_filter     = np.concatenate((oblique_weights_example_filter_ex, oblique_weights_example_filter_inh),axis=0)\n",
    "    tuft_weights_example_filter_ex  = np.fliplr(first_layer_weights[:,ex_tuft_syn_inds,selected_filter_ind].T)\n",
    "    tuft_weights_example_filter_inh = np.fliplr(first_layer_weights[:,inh_tuft_syn_inds,selected_filter_ind].T)\n",
    "    tuft_weights_example_filter     = np.concatenate((tuft_weights_example_filter_ex,tuft_weights_example_filter_inh),axis=0)\n",
    "    \n",
    "    time_axis = -np.arange(first_layer_weights.shape[0])\n",
    "    \n",
    "    upper_limit = max(np.percentile(abs(first_layer_weights[:,:,selected_filter_ind]),99.8),np.percentile(abs(first_layer_weights[:,:,selected_filter_ind]),0.2))\n",
    "    weights_ylims = np.array([-1.08,1.08]) * upper_limit\n",
    "    \n",
    "    weight_ticks_lims = (np.array(weights_ylims)/weight_granularity).astype(int) * weight_granularity\n",
    "    \n",
    "    ax00.axis('off')\n",
    "    ax01.axis('off')\n",
    "    ax02.axis('off')\n",
    "    \n",
    "    # basal\n",
    "    weights_images = ax00.imshow(resize(basal_weights_example_filter, (combined_filter.shape[0], 200)),\n",
    "                                 cmap='jet', vmin=weights_ylims[0],vmax=weights_ylims[1], aspect='auto')\n",
    "    ax00.set_xticks([])\n",
    "    ax00.set_ylabel('Synaptic index', fontsize=xylabels_fontsize)\n",
    "    for ytick_label in ax00.get_yticklabels():\n",
    "        ytick_label.set_fontsize(xytick_labels_fontsize)\n",
    "    \n",
    "    ax_colorbar = inset_axes(ax00, width=\"67%\", height=\"6%\", loc=2)\n",
    "    cbar = plt.colorbar(weights_images, cax=ax_colorbar, orientation=\"horizontal\", ticks=[weight_ticks_lims[0], 0, weight_ticks_lims[1]])\n",
    "    ax_colorbar.xaxis.set_ticks_position(\"bottom\")\n",
    "    cbar.ax.tick_params(labelsize=xytick_labels_fontsize-2)\n",
    "    ax00.text(10, 190, 'Weight (A.U)', color='k', fontsize=xytick_labels_fontsize+1, ha='left', va='top', rotation='horizontal')\n",
    "    \n",
    "    ax10.plot(time_axis, np.fliplr(basal_weights_example_filter_ex).T , c='r', alpha=all_traces_alpha)\n",
    "    ax10.plot(time_axis, np.mean(np.fliplr(basal_weights_example_filter_ex).T, axis=1) , c='r', lw=mean_linewidth)\n",
    "    ax10.plot(time_axis, np.fliplr(basal_weights_example_filter_inh).T, c='b', alpha=all_traces_alpha)\n",
    "    ax10.plot(time_axis, np.mean(np.fliplr(basal_weights_example_filter_inh).T, axis=1) , c='b', lw=mean_linewidth)\n",
    "    \n",
    "    ax10.set_xlim(time_axis.min(),time_axis.max())\n",
    "    ax10.set_ylim(weights_ylims[0],weights_ylims[1])\n",
    "    if set_ylabel:\n",
    "        ax10.set_ylabel('Weight (A.U)', fontsize=xylabels_fontsize)\n",
    "    \n",
    "    time_ticks_to_show = np.unique((np.array(time_axis)/time_granularity).astype(int) * time_granularity)\n",
    "    ax10.set_xticks(time_ticks_to_show)\n",
    "    \n",
    "    weights_axis = np.linspace(weights_ylims[0],weights_ylims[1],10)\n",
    "    weight_ticks_to_show = np.unique((np.array(weights_axis)/weight_granularity).astype(int) * weight_granularity)\n",
    "    ax10.set_yticks(weight_ticks_to_show)\n",
    "    \n",
    "    \n",
    "    ax10.spines['top'].set_visible(False)\n",
    "    ax10.spines['right'].set_visible(False)\n",
    "    \n",
    "    for ytick_label in ax10.get_yticklabels():\n",
    "        ytick_label.set_fontsize(xytick_labels_fontsize)\n",
    "    for xtick_label in ax10.get_xticklabels():\n",
    "        xtick_label.set_fontsize(xytick_labels_fontsize)\n",
    "    \n",
    "    # oblique\n",
    "    weights_images = ax01.imshow(resize(oblique_weights_example_filter, (combined_filter.shape[0], 200)),\n",
    "                                 cmap='jet', vmin=weights_ylims[0],vmax=weights_ylims[1], aspect='auto')\n",
    "    ax01.set_xticks([])\n",
    "    ax01.set_ylabel('Synaptic index', fontsize=xylabels_fontsize)\n",
    "    for ytick_label in ax01.get_yticklabels():\n",
    "        ytick_label.set_fontsize(xytick_labels_fontsize)\n",
    "    \n",
    "    ax11.plot(time_axis, np.fliplr(oblique_weights_example_filter_ex).T , c='r', alpha=all_traces_alpha)\n",
    "    ax11.plot(time_axis, np.mean(np.fliplr(oblique_weights_example_filter_ex).T, axis=1) , c='r', lw=mean_linewidth)\n",
    "    ax11.plot(time_axis, np.fliplr(oblique_weights_example_filter_inh).T, c='b', alpha=all_traces_alpha)\n",
    "    ax11.plot(time_axis, np.mean(np.fliplr(oblique_weights_example_filter_inh).T, axis=1) , c='b', lw=mean_linewidth)\n",
    "    \n",
    "    ax11.set_xlim(time_axis.min(),time_axis.max())\n",
    "    ax11.set_xlabel('Time before $t_0$ (ms)', fontsize=xylabels_fontsize);\n",
    "    ax11.set_ylim(weights_ylims[0],weights_ylims[1])\n",
    "    \n",
    "    time_ticks_to_show = np.unique((np.array(time_axis)/time_granularity).astype(int) * time_granularity)\n",
    "    ax11.set_xticks(time_ticks_to_show)\n",
    "    \n",
    "    ax11.spines['top'].set_visible(False)\n",
    "    ax11.spines['right'].set_visible(False)\n",
    "    ax11.spines['left'].set_visible(False)\n",
    "    \n",
    "    ax11.set_yticks([])\n",
    "    for ytick_label in ax11.get_yticklabels():\n",
    "        ytick_label.set_fontsize(xytick_labels_fontsize)\n",
    "    for xtick_label in ax11.get_xticklabels():\n",
    "        xtick_label.set_fontsize(xytick_labels_fontsize)\n",
    "        \n",
    "    # tuft\n",
    "    #weights_images = ax02.imshow(tuft_weights_example_filter,cmap='jet', aspect='auto')\n",
    "    weights_images = ax02.imshow(resize(tuft_weights_example_filter, (combined_filter.shape[0], 200)),\n",
    "                                 cmap='jet', vmin=weights_ylims[0],vmax=weights_ylims[1], aspect='auto')\n",
    "    ax02.set_xticks([])\n",
    "    ax02.set_ylabel('Synaptic index', fontsize=xylabels_fontsize)\n",
    "    for ytick_label in ax02.get_yticklabels():\n",
    "        ytick_label.set_fontsize(xytick_labels_fontsize)\n",
    "    \n",
    "    ax12.plot(time_axis, np.fliplr(tuft_weights_example_filter_ex).T , c='r', alpha=all_traces_alpha)\n",
    "    ax12.plot(time_axis, np.mean(np.fliplr(tuft_weights_example_filter_ex).T, axis=1) , c='r', lw=mean_linewidth)\n",
    "    ax12.plot(time_axis, np.fliplr(tuft_weights_example_filter_inh).T, c='b', alpha=all_traces_alpha)\n",
    "    ax12.plot(time_axis, np.mean(np.fliplr(tuft_weights_example_filter_inh).T, axis=1) , c='b', lw=mean_linewidth)\n",
    "    \n",
    "    ax12.set_xlim(time_axis.min(),time_axis.max())\n",
    "    ax12.set_ylim(weights_ylims[0],weights_ylims[1])\n",
    "    ax12.set_yticks([])\n",
    "    \n",
    "    time_ticks_to_show = np.unique((np.array(time_axis)/time_granularity).astype(int) * time_granularity)\n",
    "    ax12.set_xticks(time_ticks_to_show)\n",
    "    \n",
    "    ax12.spines['top'].set_visible(False)\n",
    "    ax12.spines['right'].set_visible(False)\n",
    "    ax12.spines['left'].set_visible(False)\n",
    "    \n",
    "    for ytick_label in ax12.get_yticklabels():\n",
    "        ytick_label.set_fontsize(xytick_labels_fontsize)\n",
    "    for xtick_label in ax12.get_xticklabels():\n",
    "        xtick_label.set_fontsize(xytick_labels_fontsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------\n",
      "loading testing files...\n",
      "-------------\n",
      "will be loading the following files:\n",
      "data/Data_test/sim__saved_InputSpikes_DVTs__561_outSpikes__128_simulationRuns__6_secDuration__randomSeed_100520.p\n",
      "data/Data_test/sim__saved_InputSpikes_DVTs__596_outSpikes__128_simulationRuns__6_secDuration__randomSeed_100511.p\n",
      "data/Data_test/sim__saved_InputSpikes_DVTs__632_outSpikes__128_simulationRuns__6_secDuration__randomSeed_100525.p\n",
      "data/Data_test/sim__saved_InputSpikes_DVTs__647_outSpikes__128_simulationRuns__6_secDuration__randomSeed_100519.p\n",
      "data/Data_test/sim__saved_InputSpikes_DVTs__655_outSpikes__128_simulationRuns__6_secDuration__randomSeed_100518.p\n",
      "data/Data_test/sim__saved_InputSpikes_DVTs__901_outSpikes__128_simulationRuns__6_secDuration__randomSeed_100523.p\n",
      "-------------\n",
      "-----------------------------------------------------------------\n",
      "loading file: 'data/Data_test/sim__saved_InputSpikes_DVTs__561_outSpikes__128_simulationRuns__6_secDuration__randomSeed_100520.p'\n",
      "loading took 24.082 seconds\n",
      "-----------------------------------------------------------------\n",
      "-----------------------------------------------------------------\n",
      "loading file: 'data/Data_test/sim__saved_InputSpikes_DVTs__596_outSpikes__128_simulationRuns__6_secDuration__randomSeed_100511.p'\n",
      "loading took 18.778 seconds\n",
      "-----------------------------------------------------------------\n",
      "-----------------------------------------------------------------\n",
      "loading file: 'data/Data_test/sim__saved_InputSpikes_DVTs__632_outSpikes__128_simulationRuns__6_secDuration__randomSeed_100525.p'\n",
      "loading took 36.464 seconds\n",
      "-----------------------------------------------------------------\n",
      "-----------------------------------------------------------------\n",
      "loading file: 'data/Data_test/sim__saved_InputSpikes_DVTs__647_outSpikes__128_simulationRuns__6_secDuration__randomSeed_100519.p'\n",
      "loading took 19.491 seconds\n",
      "-----------------------------------------------------------------\n",
      "-----------------------------------------------------------------\n",
      "loading file: 'data/Data_test/sim__saved_InputSpikes_DVTs__655_outSpikes__128_simulationRuns__6_secDuration__randomSeed_100518.p'\n",
      "loading took 18.104 seconds\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#%% load test dataset\n",
    "\n",
    "print('------------------------------------------------------------------------------------')\n",
    "print('loading testing files...')\n",
    "test_file_loading_start_time = time.time()\n",
    "\n",
    "v_threshold = -55\n",
    "\n",
    "# kaggle RAM only permits 6 files to be loaded at the same time, so we select only 6 \n",
    "# (the 6th file was slected specifically to contain the trace from Figure 2 that was used in the paper)\n",
    "test_files = test_files[:5] + [test_files[10]]\n",
    "\n",
    "print('-------------')\n",
    "print('will be loading the following files:')\n",
    "[print(x) for x in test_files]\n",
    "print('-------------')\n",
    "\n",
    "# load test data\n",
    "X_test , y_spike_test , y_soma_test  = parse_multiple_sim_experiment_files(test_files)\n",
    "y_soma_test[y_soma_test > v_threshold] = v_threshold\n",
    "\n",
    "test_file_loading_duration_min = (time.time() - test_file_loading_start_time)/60\n",
    "print('time took to load data is %.3f minutes' %(test_file_loading_duration_min))\n",
    "print('------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
